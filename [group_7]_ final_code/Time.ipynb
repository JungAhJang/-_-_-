{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88715b40-71dc-4b4b-88f9-a780a7423f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\anaconda\\lib\\site-packages (0.4.3.20220106)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: lxml in c:\\anaconda\\lib\\site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from snscrape) (3.3.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\anaconda\\lib\\site-packages (from snscrape) (2.26.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\anaconda\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests[socks]->snscrape) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests[socks]->snscrape) (1.26.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\anaconda\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Requirement already satisfied: konlpy in c:\\anaconda\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\anaconda\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\anaconda\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\anaconda\\lib\\site-packages (from konlpy) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "!pip install snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "!pip install konlpy\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102c264f-ec02-4157-8893-f1435a76bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by text search\n",
    "# 시작일부터 끝 날까지 데이터를 반환하는 함수\n",
    "\n",
    "def get_df(since, until):\n",
    "    maxTweets = 5000\n",
    "\n",
    "    # Creating list to append tweet data to\n",
    "    tweets_list = []\n",
    "\n",
    "\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'느린마을 막걸리 since:{since} until:{until}').get_items()):\n",
    "        if i>maxTweets:\n",
    "            break\n",
    "        tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "    # Creating a dataframe from the tweets list above\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "    \n",
    "    return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016bc022-115c-4a8f-be38-46a132d452d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_2019 = get_df('2019-01-01','2019-12-31')\n",
    "tweets_2020 = get_df('2020-01-01','2020-12-31')\n",
    "tweets_2021 = get_df('2021-01-01','2021-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba521d7-eba4-49d2-abe1-978c002f4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 속성별 빈도수 데이터 : 리스트로 가져오기\n",
    "\n",
    "# 속성별로 분류한 텍스트 데이터에서 딕셔너리 가져오는 함수 \n",
    "def get_dict(file):\n",
    "    with open(file,'r',encoding='UTF8') as f:\n",
    "        sents = []\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line: # '' 의 길이는 1, 컨텐츠가  없을 때는 0\n",
    "                break\n",
    "            sents.append(line.strip().replace(\",\",\"\").replace(\"'\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split())\n",
    "        \n",
    "        fdist = dict()\n",
    "        \n",
    "        for sent in sents:\n",
    "            fdist[sent[0]] = int(sent[1])\n",
    "        \n",
    "        sorted(fdist.items(), key=lambda x: x[1]) # 빈도수 높은 순서대로 내림차순 정렬\n",
    "        \n",
    "        return fdist\n",
    "\n",
    "#데이터 리스트로 저장\n",
    "health = list(get_dict('./data/health.txt').keys())\n",
    "quality_stability = list(get_dict('./data/quality&stability.txt').keys())\n",
    "price = list(get_dict('./data/price.txt').keys())\n",
    "flavor = list(get_dict('./data/flavor.txt').keys())\n",
    "brand = list(get_dict('./data/brand.txt').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5693e49-05a4-4251-a171-b233163027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "properties.append(health)\n",
    "properties.append(quality_stability)\n",
    "properties.append(price)\n",
    "properties.append(flavor)\n",
    "properties.append(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2590fec-5013-4b70-827a-cf95b09482e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#특수문자 불러오기\n",
    "import string\n",
    "punct = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1832216e-af3e-42b8-8886-83bd5b4eda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 불러오기\n",
    "stop_words = []\n",
    "\n",
    "with open(\"stopWords.txt\", \"r\",encoding='UTF8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "stop_words = data.split('\\n')\n",
    "\n",
    "with open(\"my_stopWords.txt\", \"r\",encoding='UTF8') as f:\n",
    "    data2 = f.read()\n",
    "\n",
    "stop_words.extend(data2.split('\\n'))\n",
    "\n",
    "# 기본 불용어 사전에 불용어 추가하기\n",
    "user_stop_words = ['amp','旅がえり','gt','느린','마을','막걸리']\n",
    "stop_words.extend(user_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b54ad5-b8f3-4905-8e7c-a0f4b1ba0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "# 문자열 문장을 넣으면 문자열에 대해 전처리한 토큰 리스트를 반환하는 함수\n",
    "\n",
    "def process_sents(sent):\n",
    "    sent = re.compile(r'https?://[a-zA-Z0-9_/:%#\\$&\\?\\(\\)~\\.=+-]*').sub('', sent) # url -> 삭제\n",
    "    sent = re.compile(r'(@)[a-zA-Z0-9_]*:*').sub('', sent) # 계정 태그(@아이디) -> 삭제\n",
    "    \n",
    "    tokens = okt.morphs(okt.normalize(sent)) # 정규화 후 형태소 분석한 토큰\n",
    "    \n",
    "    tokens = [token for token in tokens \\\n",
    "             if token not in stop_words and token not in punct\\\n",
    "              and len(token) > 1 and token.isalpha() \\\n",
    "             and 'ㅠ' not in token and 'ㅎ' not in token and 'ㅋ' not in token and 'ㅜ' not in token]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae9a7e19-1603-466e-af9b-8e9c044ad0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 년도에 따른 데이터를 넣으면, 해당 속성 단어가 해당 년도에 얼마나 언급되었는지 카운트한 리스트 반환\n",
    "\n",
    "def get_cnt_by_year(data): #tweets_2019\n",
    "    data_text = data['Text']\n",
    "    cnts = []\n",
    "    \n",
    "    for proper in properties: # 모든 속성에 대해 검색\n",
    "        proper_cnt = 0\n",
    "        for tweet in data_text: # 트윗 데이터의 한 트윗 문장에 대해 반복\n",
    "            for word in process_sents(tweet): # 전처리한 단어에 대해 반복\n",
    "                if word in proper: #만약 속성 단어가 포함되어 있다면\n",
    "                    proper_cnt += 1 # 카운트 더해주기\n",
    "        cnts.append(proper_cnt)\n",
    "    \n",
    "    return cnts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bb88600-6983-478e-815c-f124caed0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_2019 = get_cnt_by_year(tweets_2019)\n",
    "cnt_2020 = get_cnt_by_year(tweets_2020)\n",
    "cnt_2021 = get_cnt_by_year(tweets_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d0df27-e7e6-4946-9923-b6a9f8eaffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 48, 17, 179, 101]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc2c30c-82bb-4fdc-86f8-03bf106a57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 82, 35, 277, 194]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51095bc-fdf3-435e-85e7-d9e0ecdfe14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 76, 32, 291, 122]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc49abe-59dc-4117-bb99-65d4232e2c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
